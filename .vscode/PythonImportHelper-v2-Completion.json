[
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "concurrent.futures",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "ping",
        "importPath": "icmplib",
        "description": "icmplib",
        "isExtraImport": true,
        "detail": "icmplib",
        "documentation": {}
    },
    {
        "label": "exceptions",
        "importPath": "icmplib",
        "description": "icmplib",
        "isExtraImport": true,
        "detail": "icmplib",
        "documentation": {}
    },
    {
        "label": "bs4",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "bs4",
        "description": "bs4",
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "BeautifulSoup",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "BeautifulSoup",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "BeautifulSoup",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "Comment",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "BeautifulSoup",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "urlparse",
        "importPath": "urllib.parse",
        "description": "urllib.parse",
        "isExtraImport": true,
        "detail": "urllib.parse",
        "documentation": {}
    },
    {
        "label": "urljoin",
        "importPath": "urllib.parse",
        "description": "urllib.parse",
        "isExtraImport": true,
        "detail": "urllib.parse",
        "documentation": {}
    },
    {
        "label": "urljoin",
        "importPath": "urllib.parse",
        "description": "urllib.parse",
        "isExtraImport": true,
        "detail": "urllib.parse",
        "documentation": {}
    },
    {
        "label": "urlparse",
        "importPath": "urllib.parse",
        "description": "urllib.parse",
        "isExtraImport": true,
        "detail": "urllib.parse",
        "documentation": {}
    },
    {
        "label": "urlparse",
        "importPath": "urllib.parse",
        "description": "urllib.parse",
        "isExtraImport": true,
        "detail": "urllib.parse",
        "documentation": {}
    },
    {
        "label": "socket",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "socket",
        "description": "socket",
        "detail": "socket",
        "documentation": {}
    },
    {
        "label": "psutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "psutil",
        "description": "psutil",
        "detail": "psutil",
        "documentation": {}
    },
    {
        "label": "scapy.all",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "scapy.all",
        "description": "scapy.all",
        "detail": "scapy.all",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "hashlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "hashlib",
        "description": "hashlib",
        "detail": "hashlib",
        "documentation": {}
    },
    {
        "label": "bing_search",
        "importPath": "modules.build",
        "description": "modules.build",
        "isExtraImport": true,
        "detail": "modules.build",
        "documentation": {}
    },
    {
        "label": "DirectoryScanner",
        "importPath": "modules.build",
        "description": "modules.build",
        "isExtraImport": true,
        "detail": "modules.build",
        "documentation": {}
    },
    {
        "label": "send_request",
        "importPath": "modules.build",
        "description": "modules.build",
        "isExtraImport": true,
        "detail": "modules.build",
        "documentation": {}
    },
    {
        "label": "WebSpider",
        "importPath": "modules.build",
        "description": "modules.build",
        "isExtraImport": true,
        "detail": "modules.build",
        "documentation": {}
    },
    {
        "label": "send_request",
        "importPath": "modules.build",
        "description": "modules.build",
        "isExtraImport": true,
        "detail": "modules.build",
        "documentation": {}
    },
    {
        "label": "multiprocessing",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "multiprocessing",
        "description": "multiprocessing",
        "detail": "multiprocessing",
        "documentation": {}
    },
    {
        "label": "extract_summary_from_html",
        "importPath": "modules.subdomain",
        "description": "modules.subdomain",
        "isExtraImport": true,
        "detail": "modules.subdomain",
        "documentation": {}
    },
    {
        "label": "send_request",
        "importPath": "modules.subdomain",
        "description": "modules.subdomain",
        "isExtraImport": true,
        "detail": "modules.subdomain",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "signal",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "signal",
        "description": "signal",
        "detail": "signal",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "sent_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "pos_tag",
        "importPath": "nltk",
        "description": "nltk",
        "isExtraImport": true,
        "detail": "nltk",
        "documentation": {}
    },
    {
        "label": "load_json",
        "importPath": "modules._recon_",
        "description": "modules._recon_",
        "isExtraImport": true,
        "detail": "modules._recon_",
        "documentation": {}
    },
    {
        "label": "check_redirect",
        "importPath": "modules._recon_",
        "description": "modules._recon_",
        "isExtraImport": true,
        "detail": "modules._recon_",
        "documentation": {}
    },
    {
        "label": "trafilatura",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "trafilatura",
        "description": "trafilatura",
        "detail": "trafilatura",
        "documentation": {}
    },
    {
        "label": "use_config",
        "importPath": "trafilatura.settings",
        "description": "trafilatura.settings",
        "isExtraImport": true,
        "detail": "trafilatura.settings",
        "documentation": {}
    },
    {
        "label": "PlaintextParser",
        "importPath": "sumy.parsers.plaintext",
        "description": "sumy.parsers.plaintext",
        "isExtraImport": true,
        "detail": "sumy.parsers.plaintext",
        "documentation": {}
    },
    {
        "label": "Tokenizer",
        "importPath": "sumy.nlp.tokenizers",
        "description": "sumy.nlp.tokenizers",
        "isExtraImport": true,
        "detail": "sumy.nlp.tokenizers",
        "documentation": {}
    },
    {
        "label": "LsaSummarizer",
        "importPath": "sumy.summarizers.lsa",
        "description": "sumy.summarizers.lsa",
        "isExtraImport": true,
        "detail": "sumy.summarizers.lsa",
        "documentation": {}
    },
    {
        "label": "datetime",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "datetime",
        "description": "datetime",
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "FastAPI",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "WebSocket",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "HTTPException",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "Request",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "CORSMiddleware",
        "importPath": "fastapi.middleware.cors",
        "description": "fastapi.middleware.cors",
        "isExtraImport": true,
        "detail": "fastapi.middleware.cors",
        "documentation": {}
    },
    {
        "label": "StaticFiles",
        "importPath": "fastapi.staticfiles",
        "description": "fastapi.staticfiles",
        "isExtraImport": true,
        "detail": "fastapi.staticfiles",
        "documentation": {}
    },
    {
        "label": "FileResponse",
        "importPath": "fastapi.responses",
        "description": "fastapi.responses",
        "isExtraImport": true,
        "detail": "fastapi.responses",
        "documentation": {}
    },
    {
        "label": "uvicorn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "uvicorn",
        "description": "uvicorn",
        "detail": "uvicorn",
        "documentation": {}
    },
    {
        "label": "recon_1",
        "importPath": "modules.activation",
        "description": "modules.activation",
        "isExtraImport": true,
        "detail": "modules.activation",
        "documentation": {}
    },
    {
        "label": "recon_2",
        "importPath": "modules.activation",
        "description": "modules.activation",
        "isExtraImport": true,
        "detail": "modules.activation",
        "documentation": {}
    },
    {
        "label": "extract_ip_address",
        "kind": 2,
        "importPath": "modules._recon_",
        "description": "modules._recon_",
        "peekOfCode": "def extract_ip_address(sentence):\n    ip_pattern = r'\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b'\n    match = re.search(ip_pattern, sentence)\n    return match.group(0) if match else None\ndef is_target():\n    data = load_json()\n    if data[\"target\"]:\n        return True\n    else:\n        return False",
        "detail": "modules._recon_",
        "documentation": {}
    },
    {
        "label": "is_target",
        "kind": 2,
        "importPath": "modules._recon_",
        "description": "modules._recon_",
        "peekOfCode": "def is_target():\n    data = load_json()\n    if data[\"target\"]:\n        return True\n    else:\n        return False\ndef target_val():\n    data = load_json()\n    if data[\"target\"]:\n        return data[\"target\"]",
        "detail": "modules._recon_",
        "documentation": {}
    },
    {
        "label": "target_val",
        "kind": 2,
        "importPath": "modules._recon_",
        "description": "modules._recon_",
        "peekOfCode": "def target_val():\n    data = load_json()\n    if data[\"target\"]:\n        return data[\"target\"]\n    else:\n        return None\ndef return_hid():\n    with open('./data/hid.json', 'r') as file:\n        data =  json.load(file)\n    return data['hid']",
        "detail": "modules._recon_",
        "documentation": {}
    },
    {
        "label": "return_hid",
        "kind": 2,
        "importPath": "modules._recon_",
        "description": "modules._recon_",
        "peekOfCode": "def return_hid():\n    with open('./data/hid.json', 'r') as file:\n        data =  json.load(file)\n    return data['hid']\ndef dns_val():\n    data = load_json()  # Replace with actual data retrieval\n    return data.get(\"dns_host\", \"\")  # Return \"\" if 'dns_host' is missing\ndef update_hid(new_data):\n    # Load existing data from the JSON file\n    with open('./data/hid.json', 'r') as file:",
        "detail": "modules._recon_",
        "documentation": {}
    },
    {
        "label": "dns_val",
        "kind": 2,
        "importPath": "modules._recon_",
        "description": "modules._recon_",
        "peekOfCode": "def dns_val():\n    data = load_json()  # Replace with actual data retrieval\n    return data.get(\"dns_host\", \"\")  # Return \"\" if 'dns_host' is missing\ndef update_hid(new_data):\n    # Load existing data from the JSON file\n    with open('./data/hid.json', 'r') as file:\n        data = json.load(file)\n    # Update the 'hid' key with the new data\n    data['hid'] = new_data\n    # Write the updated data back to the JSON file",
        "detail": "modules._recon_",
        "documentation": {}
    },
    {
        "label": "update_hid",
        "kind": 2,
        "importPath": "modules._recon_",
        "description": "modules._recon_",
        "peekOfCode": "def update_hid(new_data):\n    # Load existing data from the JSON file\n    with open('./data/hid.json', 'r') as file:\n        data = json.load(file)\n    # Update the 'hid' key with the new data\n    data['hid'] = new_data\n    # Write the updated data back to the JSON file\n    with open('./data/hid.json', 'w') as file:\n        json.dump(data, file, indent=4)  # Added indent for better readability\ndef load_json():",
        "detail": "modules._recon_",
        "documentation": {}
    },
    {
        "label": "load_json",
        "kind": 2,
        "importPath": "modules._recon_",
        "description": "modules._recon_",
        "peekOfCode": "def load_json():\n    filename = \"./data/scan.json\"\n    if not os.path.exists(filename) or os.stat(filename).st_size == 0:\n        return {\"target\": None}  # Return a default empty structure\n    try:\n        with open(filename, 'r') as file:\n            return json.load(file)\n    except json.JSONDecodeError:\n        return {\"target\": None}  # Handle corruption safely\ndef alive_status(host):",
        "detail": "modules._recon_",
        "documentation": {}
    },
    {
        "label": "alive_status",
        "kind": 2,
        "importPath": "modules._recon_",
        "description": "modules._recon_",
        "peekOfCode": "def alive_status(host):\n    data = load_json()\n    if data.get(\"target\"):\n        try:\n            # Attempt to ping the host\n            host_response = ping(host, count=2, interval=1, timeout=2, privileged=False)\n            data[\"alive\"] = host_response.is_alive\n            save_json_content(data)\n            return True\n        except exceptions.NameLookupError:",
        "detail": "modules._recon_",
        "documentation": {}
    },
    {
        "label": "is_valid_ip",
        "kind": 2,
        "importPath": "modules._recon_",
        "description": "modules._recon_",
        "peekOfCode": "def is_valid_ip(ip):\n    octets = ip.split('.')\n    if len(octets) != 4:\n        return False\n    for octet in octets:\n        if not octet.isdigit() or not (0 <= int(octet) <= 255):\n            return False\n    return True\ndef save_to_json(ip_address):\n    content = {",
        "detail": "modules._recon_",
        "documentation": {}
    },
    {
        "label": "save_to_json",
        "kind": 2,
        "importPath": "modules._recon_",
        "description": "modules._recon_",
        "peekOfCode": "def save_to_json(ip_address):\n    content = {\n        \"target\": ip_address\n        }\n    filename = find_matching_file(\"./data/history\",return_hid())+'.json'\n    with open(filename, 'w') as file:\n        json.dump(content, file, indent=4)\n    print(f\"IP address saved to {filename}\")\ndef extract_server_info(banner):\n    \"\"\"Extract server version details from a banner.\"\"\"",
        "detail": "modules._recon_",
        "documentation": {}
    },
    {
        "label": "extract_server_info",
        "kind": 2,
        "importPath": "modules._recon_",
        "description": "modules._recon_",
        "peekOfCode": "def extract_server_info(banner):\n    \"\"\"Extract server version details from a banner.\"\"\"\n    lines = banner.split(\"\\n\")\n    for line in lines:\n        if \"Server:\" in line:\n            return line.replace(\"Server:\", \"\").strip()\n        if \"SSH-\" in line or \"HTTP/\" in line:\n            return line.strip()\n    return \"Unknown Version\"\ndef get_all_ipv4():",
        "detail": "modules._recon_",
        "documentation": {}
    },
    {
        "label": "get_all_ipv4",
        "kind": 2,
        "importPath": "modules._recon_",
        "description": "modules._recon_",
        "peekOfCode": "def get_all_ipv4():\n    \"\"\"Extract all non-local IPv4 addresses from active interfaces.\"\"\"\n    ipv4_addresses = []\n    for interface, addrs in psutil.net_if_addrs().items():\n        for addr in addrs:\n            if addr.family == socket.AF_INET and not addr.address.startswith(\"127.\"):\n                ipv4_addresses.append(addr.address)  # Collect all non-local IPv4s\n    return ipv4_addresses\ndef scan_subnet(subnet):\n    \"\"\"Scan the given subnet and return a list of active hosts.\"\"\"",
        "detail": "modules._recon_",
        "documentation": {}
    },
    {
        "label": "scan_subnet",
        "kind": 2,
        "importPath": "modules._recon_",
        "description": "modules._recon_",
        "peekOfCode": "def scan_subnet(subnet):\n    \"\"\"Scan the given subnet and return a list of active hosts.\"\"\"\n    active_hosts = []\n    arp_request = scapy.ARP(pdst=subnet)\n    broadcast = scapy.Ether(dst=\"ff:ff:ff:ff:ff:ff\")\n    arp_packet = broadcast / arp_request\n    answered_list = scapy.srp(arp_packet, timeout=1, verbose=False)[0]\n    for sent, received in answered_list:\n        active_hosts.append(received.psrc)\n    return active_hosts",
        "detail": "modules._recon_",
        "documentation": {}
    },
    {
        "label": "check_port_80",
        "kind": 2,
        "importPath": "modules._recon_",
        "description": "modules._recon_",
        "peekOfCode": "def check_port_80(ip):\n    if is_target():\n        data = load_json()\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n            sock.settimeout(3)\n            if(sock.connect_ex((ip, 80)) == 0):\n                data[\"http\"] = True\n            else:\n                data[\"http\"] = False\n            save_json_content(data)",
        "detail": "modules._recon_",
        "documentation": {}
    },
    {
        "label": "fast_scan",
        "kind": 2,
        "importPath": "modules._recon_",
        "description": "modules._recon_",
        "peekOfCode": "def fast_scan():\n    ipv4_addresses = get_all_ipv4()\n    if not ipv4_addresses:\n        print(\"Could not determine any active IPv4 addresses.\")\n        return\n    all_active_hosts = set()\n    for ipv4 in ipv4_addresses:\n        subnet = \".\".join(ipv4.split(\".\")[:3]) + \".0/24\"\n        print(f\"\\nScanning subnet: {subnet} for active hosts...\")\n        active_hosts = scan_subnet(subnet)",
        "detail": "modules._recon_",
        "documentation": {}
    },
    {
        "label": "get_http_banner",
        "kind": 2,
        "importPath": "modules._recon_",
        "description": "modules._recon_",
        "peekOfCode": "def get_http_banner(url,host):\n    headers = {\n        \"Host\": host,\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n    }\n    try:\n        url = \"http://\"+url+\"/\"\n        response = requests.get(url,headers=headers, timeout=5)\n        headers = response.headers\n        server = headers.get('Server', 'Unknown')",
        "detail": "modules._recon_",
        "documentation": {}
    },
    {
        "label": "save_json_content",
        "kind": 2,
        "importPath": "modules._recon_",
        "description": "modules._recon_",
        "peekOfCode": "def save_json_content(content):\n    with open(\"./data/scan.json\", 'w') as file:\n        json.dump(content, file, indent=4)\ndef check_redirect(ip):\n    \"\"\"Check if the IP redirects to a domain.\"\"\"\n    try:\n        response = requests.get(f\"http://{ip}\", allow_redirects=True, timeout=3)\n        final_url = response.url\n        parsed_domain = urlparse(final_url).netloc\n        if parsed_domain and parsed_domain != ip:",
        "detail": "modules._recon_",
        "documentation": {}
    },
    {
        "label": "check_redirect",
        "kind": 2,
        "importPath": "modules._recon_",
        "description": "modules._recon_",
        "peekOfCode": "def check_redirect(ip):\n    \"\"\"Check if the IP redirects to a domain.\"\"\"\n    try:\n        response = requests.get(f\"http://{ip}\", allow_redirects=True, timeout=3)\n        final_url = response.url\n        parsed_domain = urlparse(final_url).netloc\n        if parsed_domain and parsed_domain != ip:\n            return parsed_domain, True\n        soup = BeautifulSoup(response.text, \"html.parser\")\n        meta_refresh = soup.find(\"meta\", attrs={\"http-equiv\": \"refresh\"})",
        "detail": "modules._recon_",
        "documentation": {}
    },
    {
        "label": "get_content_html",
        "kind": 2,
        "importPath": "modules._recon_",
        "description": "modules._recon_",
        "peekOfCode": "def get_content_html(url):\n    output_dir =f\"./data/{url}/webpage\"\n    url = 'http://' + url + '/'\n    os.makedirs(output_dir, exist_ok=True)\n    resource_dirs = {\"css\": os.path.join(output_dir, \"css\"),\n                     \"js\": os.path.join(output_dir, \"js\"),\n                     \"img\": os.path.join(output_dir, \"img\")}\n    # Create resource directories\n    for dir_path in resource_dirs.values():\n        os.makedirs(dir_path, exist_ok=True)",
        "detail": "modules._recon_",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TIMEOUT",
        "kind": 5,
        "importPath": "modules._recon_",
        "description": "modules._recon_",
        "peekOfCode": "DEFAULT_TIMEOUT = 1\ndef extract_ip_address(sentence):\n    ip_pattern = r'\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b'\n    match = re.search(ip_pattern, sentence)\n    return match.group(0) if match else None\ndef is_target():\n    data = load_json()\n    if data[\"target\"]:\n        return True\n    else:",
        "detail": "modules._recon_",
        "documentation": {}
    },
    {
        "label": "recon_1",
        "kind": 2,
        "importPath": "modules.activation",
        "description": "modules.activation",
        "peekOfCode": "def recon_1(spider, query, target,domain):\n    \"\"\"Start directory scanning and web spidering in parallel if spider is True.\"\"\"\n    print(domain)\n    if is_valid_domain(domain) and query:\n        content = extract_summary_from_html(target)\n        send_request(\"query\",content)\n        content = extract_summary_from_html(domain)\n        send_request(\"query\",content)\n    dict_process = multiprocessing.Process(target=start_directory_scan, args=(target,))\n    spid_process = multiprocessing.Process(target=start_web_spider, args=(target,))",
        "detail": "modules.activation",
        "documentation": {}
    },
    {
        "label": "recon_2",
        "kind": 2,
        "importPath": "modules.activation",
        "description": "modules.activation",
        "peekOfCode": "def recon_2(spider, query, target):\n    if is_valid_domain(target) and query:\n       content = bing_search(target)\n       for url in content:\n           content = extract_summary_from_html(url)\n           send_request(\"query\",content)\n    url = f\"http://{target}/\"\n    dict_process = multiprocessing.Process(target=start_directory_scan, args=(url,))\n    spid_process = multiprocessing.Process(target=start_web_spider, args=(url,))\n    if spider:",
        "detail": "modules.activation",
        "documentation": {}
    },
    {
        "label": "start_directory_scan",
        "kind": 2,
        "importPath": "modules.activation",
        "description": "modules.activation",
        "peekOfCode": "def start_directory_scan(target):\n    \"\"\"Function to initialize and run DirectoryScanner.\"\"\"\n    clean_target = target.rstrip(\"/\") \n    scanner = DirectoryScanner(target_url=clean_target,)\n    scanner.run_scan()\ndef start_web_spider(target):\n    clean_target = target.rstrip(\"/\") \n    spider = WebSpider(clean_target)\n    spider.start()\ndef is_valid_domain(domain):",
        "detail": "modules.activation",
        "documentation": {}
    },
    {
        "label": "start_web_spider",
        "kind": 2,
        "importPath": "modules.activation",
        "description": "modules.activation",
        "peekOfCode": "def start_web_spider(target):\n    clean_target = target.rstrip(\"/\") \n    spider = WebSpider(clean_target)\n    spider.start()\ndef is_valid_domain(domain):\n    \"\"\"Check if the input is a valid domain name.\"\"\"\n    # Regular expression for validating a domain name\n    domain_pattern = re.compile(\n        r'^(?!-)[A-Za-z0-9-]{1,63}(?<!-)\\.'  # Start with a valid character\n        r'([A-Za-z]{2,}|[A-Za-z0-9-]{1,}\\.[A-Za-z]{2,})$'  # Top-level domain",
        "detail": "modules.activation",
        "documentation": {}
    },
    {
        "label": "is_valid_domain",
        "kind": 2,
        "importPath": "modules.activation",
        "description": "modules.activation",
        "peekOfCode": "def is_valid_domain(domain):\n    \"\"\"Check if the input is a valid domain name.\"\"\"\n    # Regular expression for validating a domain name\n    domain_pattern = re.compile(\n        r'^(?!-)[A-Za-z0-9-]{1,63}(?<!-)\\.'  # Start with a valid character\n        r'([A-Za-z]{2,}|[A-Za-z0-9-]{1,}\\.[A-Za-z]{2,})$'  # Top-level domain\n    )\n    return bool(domain_pattern.match(domain))",
        "detail": "modules.activation",
        "documentation": {}
    },
    {
        "label": "DirectoryScanner",
        "kind": 6,
        "importPath": "modules.build",
        "description": "modules.build",
        "peekOfCode": "class DirectoryScanner:\n    def __init__(self, target_url, wordlist='./data/etc/common.txt', threads=None):\n        self.target_url = target_url\n        self.wordlist = wordlist\n        self.threads = threads or (multiprocessing.cpu_count() * 2)\n        self.executor = None\n        self.results_file = \"./data/scan.json\"\n        self.shutdown_flag = False\n        # Header Rotation List\n        self.headers_list = [",
        "detail": "modules.build",
        "documentation": {}
    },
    {
        "label": "WebSpider",
        "kind": 6,
        "importPath": "modules.build",
        "description": "modules.build",
        "peekOfCode": "class WebSpider:\n    def __init__(self, start_url, max_depth=2, timeout=5, scan_file=\"./data/scan.json\"):\n        self.start_url = start_url\n        self.max_depth = max_depth\n        self.timeout = timeout\n        self.scan_file = scan_file  # JSON file to store results\n        self.visited_urls = set()  # Tracks visited pages\n        self.discovered_urls = set()  # Stores all unique URLs\n        self.discovered_files = set()  # Stores specific file paths (.php, .pdf, .js, etc.)\n        # Load existing scan data or create a new one",
        "detail": "modules.build",
        "documentation": {}
    },
    {
        "label": "bing_search",
        "kind": 2,
        "importPath": "modules.build",
        "description": "modules.build",
        "peekOfCode": "def bing_search(query, num_results=2):\n    \"\"\"Performs a Bing search and returns a list of top result URLs.\"\"\"\n    search_url = f\"https://www.bing.com/search?q={query.replace(' ', '+')}\"\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36\"\n    }\n    response = requests.get(search_url, headers=headers)\n    if response.status_code != 200:\n        return []\n    soup = BeautifulSoup(response.text, \"html.parser\")",
        "detail": "modules.build",
        "documentation": {}
    },
    {
        "label": "send_request",
        "kind": 2,
        "importPath": "modules.build",
        "description": "modules.build",
        "peekOfCode": "def send_request(type, message,status=None):\n    if status != None:\n        payload = {\"type\":type,\"value\": message,'status':status}\n    else:\n        payload = {\"type\":type,\"value\": message}\n    headers = {\"Content-Type\": \"application/json\"}\n    try:\n        response = requests.post(\"http://127.0.0.1:12531/send_message\", json=payload, headers=headers)\n        response.raise_for_status()  # Raise an error for bad responses (4xx and 5xx)\n        return response.json()  # Return the response JSON if available",
        "detail": "modules.build",
        "documentation": {}
    },
    {
        "label": "htmlRetrival",
        "kind": 2,
        "importPath": "modules.html_extraction",
        "description": "modules.html_extraction",
        "peekOfCode": "def htmlRetrival(url):\n    myheaders = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n        \"Accept-Language\": \"en-US,en;q=0.9\",\n        \"Accept-Encoding\": \"gzip, deflate, br\",\n        \"Connection\": \"keep-alive\",\n        \"Referer\": \"https://www.google.com/\",\n        \"DNT\": \"1\"  \n    }\n    data = requests.get(url, headers=myheaders, verify=False)",
        "detail": "modules.html_extraction",
        "documentation": {}
    },
    {
        "label": "extract_relevant_text",
        "kind": 2,
        "importPath": "modules.html_extraction",
        "description": "modules.html_extraction",
        "peekOfCode": "def extract_relevant_text(html_content):\n    soup = BeautifulSoup(html_content, \"html.parser\")\n    # Remove style tags\n    for tag in soup([\"style\"]):  \n        tag.extract()\n    # Extract HTML comments\n    comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n    extracted_comments = [comment.strip(\"<!-->\") for comment in comments]\n    # Extract visible text\n    text_content = soup.get_text(separator=\" \")",
        "detail": "modules.html_extraction",
        "documentation": {}
    },
    {
        "label": "extract_code_snippets",
        "kind": 2,
        "importPath": "modules.html_extraction",
        "description": "modules.html_extraction",
        "peekOfCode": "def extract_code_snippets(html_content):\n    soup = BeautifulSoup(html_content, \"html.parser\")\n    code_snippets = set()\n    for code_block in soup.find_all([\"code\", \"pre\", \"script\"]):\n        code_text = code_block.get_text(separator=\"\\n\").strip()\n        if len(code_text) > 3:\n            code_snippets.add(code_text)\n    return list(code_snippets)\n# Function to extract data using regex\ndef classify_text(text):",
        "detail": "modules.html_extraction",
        "documentation": {}
    },
    {
        "label": "classify_text",
        "kind": 2,
        "importPath": "modules.html_extraction",
        "description": "modules.html_extraction",
        "peekOfCode": "def classify_text(text):\n    names = set()\n    phone_numbers = set()\n    email_ids = set()\n    cve_data = set()\n    urls = set()\n    file_paths = set()\n    email_pattern = r\"[\\w\\.-]+@[\\w\\.-]+\\.\\w+\"\n    phone_pattern = r\"\\+?\\d{1,3}[-\\s]?\\d{10}|\\b\\d{10}\\b\"\n    cve_pattern = r\"\\bCVE-\\d{4}-\\d{4,7}\\b\"",
        "detail": "modules.html_extraction",
        "documentation": {}
    },
    {
        "label": "find_sentences_with_keywords",
        "kind": 2,
        "importPath": "modules.html_extraction",
        "description": "modules.html_extraction",
        "peekOfCode": "def find_sentences_with_keywords(text, keywords, code_snippets):\n    sentences = sent_tokenize(text)\n    # Filter out sentences that exactly match code snippets\n    filtered_sentences = [\n        sentence for sentence in sentences\n        if not any(re.fullmatch(re.escape(code.strip()), sentence.strip()) for code in code_snippets)\n    ]\n    # Find sentences containing any classified keywords\n    matched_sentences = [\n        sentence for sentence in filtered_sentences ",
        "detail": "modules.html_extraction",
        "documentation": {}
    },
    {
        "label": "process_html_content",
        "kind": 2,
        "importPath": "modules.html_extraction",
        "description": "modules.html_extraction",
        "peekOfCode": "def process_html_content(html_content, output_filename=\"output.json\"):\n    if not html_content:\n        print(\"❌ No HTML content to process.\")\n        return\n    filtered_text, extracted_comments = extract_relevant_text(html_content)\n    code_snippets = extract_code_snippets(html_content)\n    classified_data = classify_text(filtered_text)\n    keywords = (\n        classified_data[\"names\"] + \n        classified_data[\"email_ids\"] + ",
        "detail": "modules.html_extraction",
        "documentation": {}
    },
    {
        "label": "extract_summary_from_html",
        "kind": 2,
        "importPath": "modules.subdomain",
        "description": "modules.subdomain",
        "peekOfCode": "def extract_summary_from_html(url, num_sentences=5, save_path=\"./data/scan.json\", max_retries=3):\n    \"\"\"Extracts summary, headings, CVE IDs, UNIX-like paths, and links from a webpage with undetectable headers.\"\"\"\n    headers_list = [\n        {\n            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n            \"Accept-Language\": \"en-US,en;q=0.9\",\n            \"DNT\": \"1\",\n            \"Referer\": \"https://www.google.com/\",\n            \"Cache-Control\": \"no-cache\",\n            \"Pragma\": \"no-cache\"",
        "detail": "modules.subdomain",
        "documentation": {}
    },
    {
        "label": "fetch_url_p1",
        "kind": 2,
        "importPath": "modules.subdomain",
        "description": "modules.subdomain",
        "peekOfCode": "def fetch_url_p1(path,host):\n    \"\"\"Fetches a specific path on a target IP while using a custom Host header.\"\"\"\n    # Load scan.json\n    data = load_json()\n    # Extract target IP and custom host\n    target_ip = data.get(\"target\", \"\").strip()\n    # Ensure we have a valid IP and custom host\n    if not target_ip:\n        return {\"error\": \"No target IP found in scan.json\"}\n    if not host:",
        "detail": "modules.subdomain",
        "documentation": {}
    },
    {
        "label": "fetch_url",
        "kind": 2,
        "importPath": "modules.subdomain",
        "description": "modules.subdomain",
        "peekOfCode": "def fetch_url(path=\"/.git/\"):\n    \"\"\"Fetches a specific path on a target IP while using a custom Host header.\"\"\"\n    # Load scan.json\n    data = load_json()\n    # Extract target IP and custom host\n    target_ip = data.get(\"target\", \"\").strip()\n    custom_host = data.get(\"dns_host\", \"\").strip()\n    # Ensure we have a valid IP and custom host\n    if not target_ip:\n        return {\"error\": \"No target IP found in scan.json\"}",
        "detail": "modules.subdomain",
        "documentation": {}
    },
    {
        "label": "p1_url",
        "kind": 2,
        "importPath": "modules.subdomain",
        "description": "modules.subdomain",
        "peekOfCode": "def p1_url(scan_file=\"./data/scan.json\"):\n    \"\"\"Checks response status for all URLs in 'p1_link' in scan.json (sequential execution).\"\"\"\n    # Ensure scan.json exists\n    if not os.path.exists(scan_file):\n        return {\"error\": \"scan.json not found\"}\n    # Load scan.json\n    try:\n        with open(scan_file, \"r\") as file:\n            data = json.load(file)\n    except (json.JSONDecodeError, FileNotFoundError):",
        "detail": "modules.subdomain",
        "documentation": {}
    },
    {
        "label": "p1_dict",
        "kind": 2,
        "importPath": "modules.subdomain",
        "description": "modules.subdomain",
        "peekOfCode": "def p1_dict(scan_file=\"./data/scan.json\", max_threads=10):\n    \"\"\"Performs fast dictionary-based scanning using threading.\"\"\"\n    if not os.path.exists(scan_file):\n        return {\"error\": \"scan.json not found\"}\n    # Load scan.json\n    try:\n        with open(scan_file, \"r\") as file:\n            data = json.load(file)\n    except (json.JSONDecodeError, FileNotFoundError):\n        return {\"error\": \"Invalid scan.json file\"}",
        "detail": "modules.subdomain",
        "documentation": {}
    },
    {
        "label": "custom_config",
        "kind": 5,
        "importPath": "modules.subdomain",
        "description": "modules.subdomain",
        "peekOfCode": "custom_config = use_config()\n# Network settings\ncustom_config.set(\"DEFAULT\", \"DOWNLOAD_TIMEOUT\", \"30\")  # Drop request after 30 sec\ncustom_config.set(\"DEFAULT\", \"SLEEP_TIME\", \"5\")  # Time between requests\n# Input file size limits\ncustom_config.set(\"DEFAULT\", \"MAX_FILE_SIZE\", \"20000000\")  # Max input file size (20MB)\ncustom_config.set(\"DEFAULT\", \"MIN_FILE_SIZE\", \"10\")  # Min input file size\n# Extraction settings\ncustom_config.set(\"DEFAULT\", \"MIN_EXTRACTED_SIZE\", \"250\")  # Acceptable size in characters\ncustom_config.set(\"DEFAULT\", \"MIN_OUTPUT_SIZE\", \"1\")  # Absolute min text output",
        "detail": "modules.subdomain",
        "documentation": {}
    },
    {
        "label": "cleanup",
        "kind": 2,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "def cleanup():\n    \"\"\"Cleanup function to terminate all running processes.\"\"\"\n    for process in processes:\n        if process.is_alive():\n            process.terminate()\n            process.join()  # Wait for the process to terminate\n    print(\"All processes terminated.\")\ndef signal_handler(sig, frame):\n    \"\"\"Handle termination signals.\"\"\"\n    print(\"Received termination signal. Cleaning up...\")",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "signal_handler",
        "kind": 2,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "def signal_handler(sig, frame):\n    \"\"\"Handle termination signals.\"\"\"\n    print(\"Received termination signal. Cleaning up...\")\n    cleanup()\n    sys.exit(0)\nsignal.signal(signal.SIGINT, signal_handler)\ndef is_server_running(host=\"127.0.0.1\", port=12531):\n    \"\"\"Check if the server is already running to avoid multiple browser tabs.\"\"\"\n    try:\n        with socket.create_connection((host, port), timeout=2):",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "is_server_running",
        "kind": 2,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "def is_server_running(host=\"127.0.0.1\", port=12531):\n    \"\"\"Check if the server is already running to avoid multiple browser tabs.\"\"\"\n    try:\n        with socket.create_connection((host, port), timeout=2):\n            return True\n    except (socket.timeout, ConnectionRefusedError):\n        return False\ndef run_server():\n    \"\"\"Runs the FastAPI server with suppressed output.\"\"\"\n    print(\"🚀 Welcome to ONI-DIR! Please visit http://127.0.0.1:12531\")  # ✅ Custom message",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "run_server",
        "kind": 2,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "def run_server():\n    \"\"\"Runs the FastAPI server with suppressed output.\"\"\"\n    print(\"🚀 Welcome to ONI-DIR! Please visit http://127.0.0.1:12531\")  # ✅ Custom message\n    # ✅ Suppress all unwanted terminal output\n    sys.stdout = open(os.devnull, \"w\")\n    sys.stderr = open(os.devnull, \"w\")\n    # Start server\n    uvicorn.run(\"app:app\", host=\"127.0.0.1\", port=12531, log_level=\"critical\")\nif __name__ == \"__main__\":\n    run_server()",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "app = FastAPI()\n# Detect if running inside PyInstaller\nif getattr(sys, 'frozen', False):\n    BASE_DIR = sys._MEIPASS  # PyInstaller temp directory\nelse:\n    BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n# Correctly locate `static/` directory\nSTATIC_DIR = os.path.join(BASE_DIR, \"static\")\nDATA_DIR = os.path.join(BASE_DIR, \"data\")\n# Ensure static directory exists before mounting",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "STATIC_DIR",
        "kind": 5,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "STATIC_DIR = os.path.join(BASE_DIR, \"static\")\nDATA_DIR = os.path.join(BASE_DIR, \"data\")\n# Ensure static directory exists before mounting\nif os.path.exists(STATIC_DIR):\n    app.mount(\"/static\", StaticFiles(directory=STATIC_DIR), name=\"static\")\nelse:\n    print(f\"⚠️ WARNING: Static directory '{STATIC_DIR}' not found!\")\n# Enable CORS for frontend\nscan_file = \"./data/scan.json\"\n# Enable CORS for frontend",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "DATA_DIR",
        "kind": 5,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n# Ensure static directory exists before mounting\nif os.path.exists(STATIC_DIR):\n    app.mount(\"/static\", StaticFiles(directory=STATIC_DIR), name=\"static\")\nelse:\n    print(f\"⚠️ WARNING: Static directory '{STATIC_DIR}' not found!\")\n# Enable CORS for frontend\nscan_file = \"./data/scan.json\"\n# Enable CORS for frontend\napp.add_middleware(",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "scan_file",
        "kind": 5,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "scan_file = \"./data/scan.json\"\n# Enable CORS for frontend\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],  # Allow frontend origin\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\nactive_connections = []",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "active_connections",
        "kind": 5,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "active_connections = []\nprocesses = []\n@app.get(\"/\")\nasync def root():\n    return FileResponse(\"static/index.html\")  # Serve index.html properly\n@app.websocket(\"/ws\")\nasync def websocket_endpoint(websocket: WebSocket):\n    \"\"\"Handles WebSocket connections\"\"\"\n    await websocket.accept()\n    active_connections.append(websocket)",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "processes",
        "kind": 5,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "processes = []\n@app.get(\"/\")\nasync def root():\n    return FileResponse(\"static/index.html\")  # Serve index.html properly\n@app.websocket(\"/ws\")\nasync def websocket_endpoint(websocket: WebSocket):\n    \"\"\"Handles WebSocket connections\"\"\"\n    await websocket.accept()\n    active_connections.append(websocket)\n    print(\"Client connected!\")",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "scan_file",
        "kind": 5,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "scan_file = \"./data/scan.json\"\nprocesses = []  # Track active processes\n@app.post(\"/submit_target\")\nasync def submit_target(request: Request):\n    \"\"\"Submit target details, store them in scan.json, and start scanning processes.\"\"\"\n    data = await request.json()\n    name = data.get(\"name\")\n    target = data.get(\"target\")\n    spider = data.get(\"spider\")\n    query_search = data.get(\"query_search\")",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "processes",
        "kind": 5,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "processes = []  # Track active processes\n@app.post(\"/submit_target\")\nasync def submit_target(request: Request):\n    \"\"\"Submit target details, store them in scan.json, and start scanning processes.\"\"\"\n    data = await request.json()\n    name = data.get(\"name\")\n    target = data.get(\"target\")\n    spider = data.get(\"spider\")\n    query_search = data.get(\"query_search\")\n    if not name or not target:",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "is_valid_domain",
        "kind": 2,
        "importPath": "test",
        "description": "test",
        "peekOfCode": "def is_valid_domain(domain):\n    \"\"\"Check if the input is a valid domain name.\"\"\"\n    # Regular expression for validating a domain name\n    domain_pattern = re.compile(\n        r'^(?!-)[A-Za-z0-9-]{1,63}(?<!-)\\.'  # Start with a valid character\n        r'([A-Za-z]{2,}|[A-Za-z0-9-]{1,}\\.[A-Za-z]{2,})$'  # Top-level domain\n    )\n    return bool(domain_pattern.match(domain))\nprint(is_valid_domain(\"192.168.0.2\"))",
        "detail": "test",
        "documentation": {}
    }
]